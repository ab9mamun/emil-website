---
title: "The Future of Continual Learning in the Era of Foundation Models: Three Key Directions"
abstract: "Continual learning—the ability to acquire, retain, and refine knowledge over time—has always been fundamental to intelligence, both human and artificial. Historically, different AI paradigms have acknowledged this need, albeit with varying priorities: early expert and production systems focused on incremental knowledge consolidation, while reinforcement learning emphasised dynamic adaptation. With the rise of deep learning, deep continual learning has primarily focused on learning robust and reusable representations over time to solve sequences of increasingly complex tasks. However, the emergence of Large Language Models (LLMs) and foundation models has raised the question: Do we still need continual learning when centralised, monolithic models can tackle diverse tasks with access to internet-scale knowledge? We argue that continual learning remains essential for three key reasons: (i) continual pre-training is still necessary to ensure foundation models remain up to date, mitigating knowledge staleness and distribution shifts while integrating new information; (ii) continual fine-tuning enables models to specialise and personalise, adapting to domain-specific tasks, user preferences, and real-world constraints without full retraining, avoiding the need for computationally expensive long context-windows; (iii) continual compositionality offers a scalable and modular approach to intelligence, enabling the orchestration of foundation models and agents to be dynamically composed, recombined, and adapted. While continual pre- training and fine-tuning are explored as niche research directions, we argue it is continual compositionality that will mark the rebirth of continual learning. The future of AI will not be defined by a single static model but by an ecosystem of continually evolving and interacting models, making continual learning more relevant than eve"

summary: "Continual learning has always been central to intelligence, enabling systems to retain and refine knowledge over time. While foundation models and LLMs can handle diverse tasks, continual learning remains crucial. It prevents knowledge from becoming stale through continual pre-training, supports adaptation and personalization via continual fine-tuning, and enables scalable, modular intelligence through continual compositionality. Ultimately, the future of AI will be shaped not by static monolithic models, but by an evolving ecosystem of continually adapting and interacting models."

location: Online (Zoom)
date: 2025-09-24T12:00:00-07:00
date_end: 2025-09-24T12:30:00-07:00
all_day: false
links:
  - url: https://arxiv.org/pdf/2506.03320
    name: "Paper"
  - url: pres.pptx
    name: "slides"
event: EMIL Spring'25 Seminars
event_url: " "
publishDate: 2025-09-25T13:45:00-07:00
draft: false
featured: false
authors:
  - reza-rahimi-azghan
image:
  filename: featured.png
  focal_point: Smart
  preview_only: false
---
