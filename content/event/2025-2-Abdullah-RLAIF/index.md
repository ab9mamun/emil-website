---
title: "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"

abstract: "Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards self-improvement by demonstrating that RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF."

summary: "Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, this papers show that RLAIF achieves comparable performance to RLHF."

location: Online (Zoom)
date: 2025-02-21T09:00:00.000Z
date_end: 2025-02-21T09:30:00.000Z
all_day: false
links:
  - url: https://arxiv.org/abs/2309.00267
    name: "Paper"
  - url: rlaif_mamun.pdf
    name: "slides"
event: EMIL Spring'25 RL Seminars
event_url: " "
publishDate: 2025-02-21T08:00:00.000Z
draft: false
featured: false
authors:
  - abdullah-mamun
image:
  filename: featured.png
  focal_point: Smart
  preview_only: false
---
