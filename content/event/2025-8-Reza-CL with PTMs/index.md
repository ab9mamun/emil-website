---
title: "Continual Learning with Pre-Trained Models: A Survey"
abstract: "Nowadays, real-world applications often face streaming data, which requires the learning system to absorb new knowledge as data evolves. Contin- ual Learning (CL) aims to achieve this goal and meanwhile overcome the catastrophic forgetting of former knowledge when learning new ones. Typ- ical CL methods build the model from scratch to grow with incoming data. However, the advent of the pre-trained model (PTM) era has sparked immense research interest, particularly in leverag- ing PTMsâ€™ robust representational capabilities for CL. This paper presents a comprehensive survey of the latest advancements in PTM-based CL. We categorize existing methodologies into three dis- tinct groups, providing a comparative analysis of their similarities, differences, and respective ad- vantages and disadvantages. Additionally, we of- fer an empirical study contrasting various state- of-the-art methods to highlight concerns regard- ing fairness in comparisons. The source code to reproduce these evaluations is available at: https: //github.com/sun-hailong/LAMDA-PILOT."

summary: "This survey reviews continual learning (CL) with pre-trained models (PTMs), grouping methods into prompt-based, representation-based, and model mixture-based approaches. It finds representation-based baselines often outperform more complex prompt strategies and flags fairness issues in some evaluations. Key challenges include handling domain gaps, ensuring computational efficiency, expanding to multimodal and language models, and creating benchmarks beyond PTM pretraining data."

location: Online (Zoom)
date: 2025-08-13T12:00:00-07:00
date_end: 2025-08-13T12:30:00-07:00
all_day: false
links:
  - url: https://arxiv.org/pdf/2401.16386
    name: "Paper"
  - url: pres.pptx
    name: "slides"
event: EMIL Spring'25 Seminars
event_url: " "
publishDate: 2025-08-13T13:45:00-07:00
draft: false
featured: false
authors:
  - reza-rahimi-azghan
image:
  filename: featured.png
  focal_point: Smart
  preview_only: false
---
