---
title: "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
abstract: "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies"

summary: "An algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning."
location: Online (Zoom)
date: 2022-12-14T12:00:00.000Z
date_end: 2022-12-14T12:30:00.000Z
all_day: false
links:
  - url: https://arxiv.org/abs/1703.03400
    name: "PDF"
  - url: slides.pdf
    name: "slides"
event: EMIL Fall'22 Seminars
event_url: " "
publishDate: 2022-12-14T20:56:00.000Z
draft: false
featured: false
authors:
  - chia-cheng-kuo
image:
  filename: featured.png
  focal_point: Smart
  preview_only: false
---
